{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, EsmForProteinFolding\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from Bio import AlignIO\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForProteinFolding were not initialized from the model checkpoint at facebook/esmfold_v1 and are newly initialized: ['esm.contact_head.regression.bias', 'esm.contact_head.regression.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EsmForProteinFolding(\n",
       "  (esm): EsmModel(\n",
       "    (embeddings): EsmEmbeddings(\n",
       "      (word_embeddings): Embedding(33, 2560, padding_idx=1)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (position_embeddings): Embedding(1026, 2560, padding_idx=1)\n",
       "    )\n",
       "    (encoder): EsmEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-35): 36 x EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (key): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (value): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (emb_layer_norm_after): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (contact_head): EsmContactPredictionHead(\n",
       "      (regression): Linear(in_features=1440, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (esm_s_mlp): Sequential(\n",
       "    (0): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=2560, out_features=1024, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (embedding): Embedding(23, 1024, padding_idx=0)\n",
       "  (trunk): EsmFoldingTrunk(\n",
       "    (pairwise_positional_embedding): EsmFoldRelativePosition(\n",
       "      (embedding): Embedding(66, 128)\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-47): 48 x EsmFoldTriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): EsmFoldSequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): EsmFoldPairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): EsmFoldSelfAttention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): EsmFoldTriangleMultiplicativeUpdate(\n",
       "          (linear_a_p): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (layer_norm_out): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): EsmFoldTriangleMultiplicativeUpdate(\n",
       "          (linear_a_p): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (layer_norm_out): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): EsmFoldTriangleAttention(\n",
       "          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): EsmFoldLinear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): EsmFoldAttention(\n",
       "            (linear_q): EsmFoldLinear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): EsmFoldLinear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): EsmFoldLinear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): EsmFoldTriangleAttention(\n",
       "          (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): EsmFoldLinear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): EsmFoldAttention(\n",
       "            (linear_q): EsmFoldLinear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): EsmFoldLinear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): EsmFoldLinear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): EsmFoldResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): EsmFoldResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): EsmFoldDropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): EsmFoldDropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (recycle_s_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (recycle_z_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (recycle_disto): Embedding(15, 128)\n",
       "    (structure_module): EsmFoldStructureModule(\n",
       "      (layer_norm_s): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_z): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear_in): EsmFoldLinear(in_features=384, out_features=384, bias=True)\n",
       "      (ipa): EsmFoldInvariantPointAttention(\n",
       "        (linear_q): EsmFoldLinear(in_features=384, out_features=192, bias=True)\n",
       "        (linear_kv): EsmFoldLinear(in_features=384, out_features=384, bias=True)\n",
       "        (linear_q_points): EsmFoldLinear(in_features=384, out_features=144, bias=True)\n",
       "        (linear_kv_points): EsmFoldLinear(in_features=384, out_features=432, bias=True)\n",
       "        (linear_b): EsmFoldLinear(in_features=128, out_features=12, bias=True)\n",
       "        (linear_out): EsmFoldLinear(in_features=2112, out_features=384, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (softplus): Softplus(beta=1.0, threshold=20.0)\n",
       "      )\n",
       "      (ipa_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm_ipa): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (transition): EsmFoldStructureModuleTransition(\n",
       "        (layers): ModuleList(\n",
       "          (0): EsmFoldStructureModuleTransitionLayer(\n",
       "            (linear_1): EsmFoldLinear(in_features=384, out_features=384, bias=True)\n",
       "            (linear_2): EsmFoldLinear(in_features=384, out_features=384, bias=True)\n",
       "            (linear_3): EsmFoldLinear(in_features=384, out_features=384, bias=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (bb_update): EsmFoldBackboneUpdate(\n",
       "        (linear): EsmFoldLinear(in_features=384, out_features=6, bias=True)\n",
       "      )\n",
       "      (angle_resnet): EsmFoldAngleResnet(\n",
       "        (linear_in): EsmFoldLinear(in_features=384, out_features=128, bias=True)\n",
       "        (linear_initial): EsmFoldLinear(in_features=384, out_features=128, bias=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x EsmFoldAngleResnetBlock(\n",
       "            (linear_1): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_2): EsmFoldLinear(in_features=128, out_features=128, bias=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (linear_out): EsmFoldLinear(in_features=128, out_features=14, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (trunk2sm_s): Linear(in_features=1024, out_features=384, bias=True)\n",
       "    (trunk2sm_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (distogram_head): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (ptm_head): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (lm_head): Linear(in_features=1024, out_features=23, bias=True)\n",
       "  (lddt_head): Sequential(\n",
       "    (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=384, out_features=128, bias=True)\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): Linear(in_features=128, out_features=1850, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EsmForProteinFolding.from_pretrained(\"facebook/esmfold_v1\")\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QTVRIKENVPVGTKTIGYKAYDPETGSSSGIRYKKSSDPEGWVDVDKNSGVITILKRLDREARSGVYNISIIASDKDGRTCNGVLGIVLE'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab a sequence from sample cadherin MSA\n",
    "cadherin_msa_path = os.path.join('..', 'data', 'cadherin', 'PF00028.alignment.seed')\n",
    "alignment = AlignIO.read(cadherin_msa_path, 'stockholm')\n",
    "seqs = []\n",
    "for record in alignment:\n",
    "    seqs.append(str(record.seq).replace('-', ''))\n",
    "\n",
    "seq = seqs[0]\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 5, 16, 19,  1,  9, 11,  6,  2, 19, 14, 19,  7, 16, 11, 16,  9,  7, 18,\n",
       "         11,  0, 18,  3, 14,  6, 16,  7, 15, 15, 15,  7,  9,  1, 18, 11, 11, 15,\n",
       "         15,  3, 14,  6,  7, 17, 19,  3, 19,  3, 11,  2, 15,  7, 19,  9, 16,  9,\n",
       "         10, 11,  1, 10,  3,  1,  6,  0,  1, 15,  7, 19, 18,  2,  9, 15,  9,  9,\n",
       "          0, 15,  3, 11,  3,  7,  1, 16,  4,  2,  7, 19, 10,  7,  9, 19, 10,  6]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esmfold_v1\")\n",
    "inputs = tokenizer([seq], return_tensors=\"pt\", add_special_tokens=False)  # A tiny random peptide\n",
    "inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from ColabFold notebook: https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/ESMFold.ipynb#scrollTo=CcyNpAvhTX6q\n",
    "def parse_output(output):\n",
    "  pae = (output[\"aligned_confidence_probs\"][0].detach().cpu() * np.arange(64)).mean(-1) * 31\n",
    "  plddt = output[\"plddt\"][0,:,1].detach().cpu()\n",
    "\n",
    "  bins = np.append(0,np.linspace(2.3125,21.6875,63))\n",
    "  sm_contacts = softmax(output[\"distogram_logits\"].detach().cpu(),-1)[0]\n",
    "\n",
    "  # NOTE: pretty sure this just looks for all distances <8 angstroms?  \n",
    "  sm_contacts = sm_contacts[...,bins<8].sum(-1)\n",
    "  xyz = output[\"positions\"][-1,0,:,1].detach().cpu()\n",
    "  mask = (output[\"atom37_atom_exists\"][0,:,1] == 1).detach().cpu()\n",
    "  o = {\"pae\":pae[mask,:][:,mask],\n",
    "       \"plddt\":plddt[mask],\n",
    "       \"sm_contacts\":sm_contacts[mask,:][:,mask],\n",
    "       \"xyz\":xyz[mask]}\n",
    "  \n",
    "  return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_261695/1320841662.py:3: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  pae = (output[\"aligned_confidence_probs\"][0].detach().cpu() * np.arange(64)).mean(-1) * 31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pae': tensor([[3.6453e-03, 1.1457e+00, 2.3981e+00,  ..., 3.0285e+00, 3.3765e+00,\n",
       "          5.9505e+00],\n",
       "         [6.6039e-01, 1.1372e-04, 6.5739e-01,  ..., 1.5370e+00, 2.2050e+00,\n",
       "          2.4775e+00],\n",
       "         [1.2210e+00, 5.7133e-01, 3.8820e-05,  ..., 9.4825e-01, 1.1879e+00,\n",
       "          1.5378e+00],\n",
       "         ...,\n",
       "         [1.8017e+00, 1.2327e+00, 9.1236e-01,  ..., 8.8048e-06, 5.2342e-01,\n",
       "          8.9983e-01],\n",
       "         [2.3614e+00, 1.6779e+00, 1.4129e+00,  ..., 5.2969e-01, 1.3646e-05,\n",
       "          6.1498e-01],\n",
       "         [6.5776e+00, 4.7676e+00, 3.7720e+00,  ..., 1.7073e+00, 7.3644e-01,\n",
       "          3.0903e-04]], dtype=torch.float64),\n",
       " 'plddt': tensor([0.7583, 0.8268, 0.8352, 0.8453, 0.8415, 0.8417, 0.8347, 0.8235, 0.8495,\n",
       "         0.8637, 0.8720, 0.8816, 0.8722, 0.8666, 0.7781, 0.7389, 0.7154, 0.7719,\n",
       "         0.8248, 0.8525, 0.8579, 0.8505, 0.8376, 0.8366, 0.8268, 0.8305, 0.8501,\n",
       "         0.8377, 0.8228, 0.8411, 0.8860, 0.9037, 0.9015, 0.8958, 0.8795, 0.8751,\n",
       "         0.8485, 0.8323, 0.7893, 0.8000, 0.8353, 0.8421, 0.8701, 0.8849, 0.8855,\n",
       "         0.9003, 0.9002, 0.8812, 0.8636, 0.8723, 0.8833, 0.8753, 0.8925, 0.8696,\n",
       "         0.8734, 0.8429, 0.8229, 0.8123, 0.8113, 0.8106, 0.8071, 0.7798, 0.7386,\n",
       "         0.7464, 0.7953, 0.8296, 0.8247, 0.8429, 0.8530, 0.8863, 0.8860, 0.9030,\n",
       "         0.8962, 0.8981, 0.8730, 0.8522, 0.8517, 0.8451, 0.8663, 0.8737, 0.8708,\n",
       "         0.8585, 0.8489, 0.8542, 0.8412, 0.8424, 0.8363, 0.8491, 0.8351, 0.8219]),\n",
       " 'sm_contacts': array([[1.0000000e+00, 9.9994224e-01, 8.8756698e-01, ..., 5.9885546e-03,\n",
       "         5.2144169e-04, 9.7415084e-04],\n",
       "        [9.9994224e-01, 1.0000000e+00, 9.9997872e-01, ..., 9.8105997e-01,\n",
       "         5.4218003e-04, 7.3163002e-04],\n",
       "        [8.8756698e-01, 9.9997872e-01, 1.0000000e+00, ..., 9.9179685e-01,\n",
       "         1.9427864e-03, 1.4877091e-03],\n",
       "        ...,\n",
       "        [5.9885546e-03, 9.8105997e-01, 9.9179685e-01, ..., 1.0000000e+00,\n",
       "         9.9999678e-01, 9.9157101e-01],\n",
       "        [5.2144169e-04, 5.4218003e-04, 1.9427864e-03, ..., 9.9999678e-01,\n",
       "         1.0000000e+00, 9.9983257e-01],\n",
       "        [9.7415084e-04, 7.3163002e-04, 1.4877091e-03, ..., 9.9157101e-01,\n",
       "         9.9983257e-01, 1.0000000e+00]], dtype=float32),\n",
       " 'xyz': tensor([[  7.4314,  -5.9036,  -0.3333],\n",
       "         [  8.3740,  -3.9000,   2.7773],\n",
       "         [  8.6613,  -0.1613,   2.9294],\n",
       "         [ 10.2281,   1.8890,   5.6563],\n",
       "         [  8.5215,   5.0889,   6.6391],\n",
       "         [  9.5078,   7.6720,   9.1855],\n",
       "         [  6.9930,   8.0682,  11.9877],\n",
       "         [  6.8713,  11.8606,  11.5340],\n",
       "         [  5.7685,  11.7287,   7.9434],\n",
       "         [  2.8013,  14.1033,   7.4658],\n",
       "         [ -0.6700,  12.7465,   6.8886],\n",
       "         [ -1.4322,  12.7570,   3.2198],\n",
       "         [  2.0211,  11.7142,   2.2530],\n",
       "         [  2.0370,   9.5340,  -0.8024],\n",
       "         [  4.2416,   6.7135,  -0.2897],\n",
       "         [  5.1969,   3.8456,  -2.6504],\n",
       "         [  3.0413,   2.7017,  -5.5583],\n",
       "         [  1.8899,  -0.8323,  -5.3806],\n",
       "         [  1.7881,  -1.6945,  -9.0036],\n",
       "         [  0.3463,  -4.9313, -10.4239],\n",
       "         [  0.6390,  -6.1291, -14.0151],\n",
       "         [ -1.4505,  -8.3355, -16.1255],\n",
       "         [  0.8878, -11.3060, -16.8603],\n",
       "         [ -0.6820, -11.8628, -20.3018],\n",
       "         [ -0.2625,  -8.3086, -21.5564],\n",
       "         [  2.4464,  -6.9543, -19.2057],\n",
       "         [  0.3294,  -3.8584, -18.8773],\n",
       "         [ -0.9098,  -2.1680, -15.7385],\n",
       "         [ -3.6443,  -0.5192, -17.7788],\n",
       "         [ -7.0775,  -1.3792, -16.4215],\n",
       "         [ -5.8054,  -2.7234, -13.1307],\n",
       "         [ -7.8434,  -1.5491, -10.1384],\n",
       "         [ -6.6394,  -1.5820,  -6.5560],\n",
       "         [ -8.5154,  -2.0263,  -3.3575],\n",
       "         [ -7.4216,  -2.1250,   0.2447],\n",
       "         [ -8.4010,  -5.5013,   1.6925],\n",
       "         [ -6.8940,  -5.1736,   5.1978],\n",
       "         [ -5.6968,  -2.2829,   7.1479],\n",
       "         [ -6.5042,  -3.0665,  10.8053],\n",
       "         [ -5.3077,   0.3770,  12.0800],\n",
       "         [ -6.5746,   2.4049,   9.1494],\n",
       "         [ -3.1022,   3.9961,   8.5733],\n",
       "         [ -3.2865,   4.0580,   4.8485],\n",
       "         [ -5.4310,   4.4562,   1.8013],\n",
       "         [ -4.8351,   3.0428,  -1.6653],\n",
       "         [ -5.5993,   4.8790,  -4.8988],\n",
       "         [ -7.9566,   2.7162,  -6.9370],\n",
       "         [ -6.3329,   3.5391, -10.2901],\n",
       "         [ -2.6519,   4.0880,  -9.4266],\n",
       "         [ -2.1648,   1.7869,  -6.5090],\n",
       "         [ -0.4894,   4.7247,  -4.6899],\n",
       "         [ -0.6114,   4.4643,  -0.9960],\n",
       "         [ -1.3181,   7.5188,   1.1053],\n",
       "         [ -0.7551,   7.8462,   4.7974],\n",
       "         [ -4.0499,   8.6819,   6.5537],\n",
       "         [ -2.9671,   8.7906,  10.2133],\n",
       "         [ -0.0604,   9.9648,  12.0901],\n",
       "         [  2.2752,   7.1024,  12.7418],\n",
       "         [  3.7675,   6.3448,  16.2208],\n",
       "         [  6.7559,   3.9103,  16.2501],\n",
       "         [  6.5416,   3.5722,  20.0937],\n",
       "         [  2.9185,   2.5383,  19.8917],\n",
       "         [  3.8042,  -0.2565,  17.6722],\n",
       "         [  7.0590,  -2.2939,  16.8766],\n",
       "         [  7.6120,  -4.5744,  13.9038],\n",
       "         [  6.1308,  -4.9126,  10.4124],\n",
       "         [  2.7098,  -3.4819,   9.7053],\n",
       "         [  1.0435,  -5.5280,   6.9782],\n",
       "         [ -1.4798,  -4.1171,   4.5962],\n",
       "         [ -3.3155,  -6.2937,   2.2005],\n",
       "         [ -4.1136,  -5.0019,  -1.2573],\n",
       "         [ -6.1305,  -6.5594,  -3.9925],\n",
       "         [ -5.4407,  -5.8699,  -7.5833],\n",
       "         [ -7.9959,  -6.7158, -10.1869],\n",
       "         [ -7.9540,  -6.5968, -13.8978],\n",
       "         [-10.8392,  -5.9284, -16.4247],\n",
       "         [-11.7889,  -9.6088, -16.5313],\n",
       "         [-12.2606,  -9.6942, -12.7047],\n",
       "         [ -9.1951, -11.7771, -11.9701],\n",
       "         [ -7.6764, -10.7307,  -8.7007],\n",
       "         [ -4.3336, -10.9793,  -7.0355],\n",
       "         [ -3.5770, -10.3487,  -3.4311],\n",
       "         [ -0.5002,  -8.4701,  -2.4043],\n",
       "         [  0.8928,  -7.6956,   1.0140],\n",
       "         [  2.6873,  -4.4938,   1.7347],\n",
       "         [  4.7750,  -4.4283,   4.8798],\n",
       "         [  5.3879,  -1.0985,   6.5485],\n",
       "         [  8.1778,  -0.5734,   9.0267],\n",
       "         [  8.2658,   2.5850,  11.1205],\n",
       "         [ 11.6052,   4.2440,  11.5558]])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_output = parse_output(output)\n",
    "parsed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pae', 'plddt', 'sm_contacts', 'xyz'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pae, plddt == measures of confidence in positions of atoms in structure\n",
    "parsed_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 90)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape LxL with probabilities of contacts between residue i,j\n",
    "parsed_output['sm_contacts'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000000e+00, 9.9994224e-01, 8.8756698e-01, ..., 5.9885546e-03,\n",
       "        5.2144169e-04, 9.7415084e-04],\n",
       "       [9.9994224e-01, 1.0000000e+00, 9.9997872e-01, ..., 9.8105997e-01,\n",
       "        5.4218003e-04, 7.3163002e-04],\n",
       "       [8.8756698e-01, 9.9997872e-01, 1.0000000e+00, ..., 9.9179685e-01,\n",
       "        1.9427864e-03, 1.4877091e-03],\n",
       "       ...,\n",
       "       [5.9885546e-03, 9.8105997e-01, 9.9179685e-01, ..., 1.0000000e+00,\n",
       "        9.9999678e-01, 9.9157101e-01],\n",
       "       [5.2144169e-04, 5.4218003e-04, 1.9427864e-03, ..., 9.9999678e-01,\n",
       "        1.0000000e+00, 9.9983257e-01],\n",
       "       [9.7415084e-04, 7.3163002e-04, 1.4877091e-03, ..., 9.9157101e-01,\n",
       "        9.9983257e-01, 1.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_output['sm_contacts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(msa_file='/nfshomes/vla/cmsc702-protein-lm/data/cadherin/PF00028.alignment.seed', plm_model='facebook/esmfold_v1', batch_size=16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, EsmForProteinFolding\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "\n",
    "from Bio import AlignIO\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Custom pytorch dataset for holding raw sequence data\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, msa_file):\n",
    "        super().__init__()\n",
    "        self.msa_file = msa_file\n",
    "        # self.tokenizer = tokenizer\n",
    "        self.seqs = self._get_sequences()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.seqs[idx]\n",
    "        return item\n",
    "\n",
    "    def _get_sequences(self):\n",
    "        alignment = AlignIO.read(self.msa_file, 'stockholm')\n",
    "        seqs = []\n",
    "        for record in alignment:\n",
    "            seqs.append(str(record.seq).replace('-', ''))\n",
    "        \n",
    "        return seqs\n",
    "\n",
    "# adapted from ColabFold notebook: https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/ESMFold.ipynb#scrollTo=CcyNpAvhTX6q\n",
    "def parse_output(output, batch):\n",
    "    # pae = (output[\"aligned_confidence_probs\"][0].detach().cpu() * np.arange(64)).mean(-1) * 31\n",
    "    # plddt = output[\"plddt\"][0,:,1].detach().cpu()\n",
    "\n",
    "    # bins = np.append(0,np.linspace(2.3125,21.6875,63))\n",
    "    # sm_contacts = softmax(output[\"distogram_logits\"].detach().cpu(),-1)[0]\n",
    "\n",
    "    # # NOTE: pretty sure this just looks for all distances <8 angstroms?  \n",
    "    # sm_contacts = sm_contacts[...,bins<8].sum(-1)\n",
    "    # xyz = output[\"positions\"][-1,0,:,1].detach().cpu()\n",
    "    # mask = (output[\"atom37_atom_exists\"][0,:,1] == 1).detach().cpu()\n",
    "    # o = {\"pae\":pae[mask,:][:,mask],\n",
    "    #     \"plddt\":plddt[mask],\n",
    "    #     \"sm_contacts\":sm_contacts[mask,:][:,mask],\n",
    "    #     \"xyz\":xyz[mask]}\n",
    "\n",
    "    bins = np.append(0,np.linspace(2.3125,21.6875,63))\n",
    "    sm_contacts = softmax(output[\"distogram_logits\"].detach().cpu(),-1)\n",
    "    sm_contacts = sm_contacts[...,bins<8].sum(-1)\n",
    "    mask = (output[\"atom37_atom_exists\"][:,:,1] == 1).detach().cpu()\n",
    "\n",
    "    contacts = []\n",
    "\n",
    "    # over each protein sequence in batch\n",
    "    for batch_idx in range(sm_contacts.shape[0]):\n",
    "        contacts_matrix = sm_contacts[batch_idx][mask[batch_idx],:][:,mask[batch_idx]]\n",
    "        contacts.append(contacts_matrix)\n",
    "\n",
    "        # assert that if protein sequence is size L, then contacts_matrix is LxL\n",
    "        assert len(batch[batch_idx]) == contacts_matrix.shape[0]\n",
    "\n",
    "    return contacts\n",
    "\n",
    "# Compute contact matrix for each sequence in MSA using PL model \n",
    "def plm_inference_per_sequence(model, \n",
    "                               tokenizer, \n",
    "                               msa_file, \n",
    "                               batch_size,\n",
    "                               device):\n",
    "\n",
    "    # setup dataset and dataloader\n",
    "    dataset = SequenceDataset(msa_file)\n",
    "    eval_dataloader = DataLoader(dataset, batch_size = batch_size)\n",
    "    \n",
    "    contact_matrices = []\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(tqdm(eval_dataloader, f\"PLM Inference on {os.path.basename(msa_file)}\")):\n",
    "            inputs = tokenizer(batch, \n",
    "                               return_tensors = 'pt', \n",
    "                               padding = True,\n",
    "                            #    truncation=True,\n",
    "                               add_special_tokens=False)\n",
    "\n",
    "            inputs.to(device)\n",
    "\n",
    "            out = model(**inputs)\n",
    "            contacts = parse_output(out, batch)\n",
    "            contact_matrices.extend(contacts)\n",
    "    \n",
    "    return contact_matrices\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--msa_file',\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"Path of MSA (Stockholm format) file for inference\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--plm_model',\n",
    "    type = str,\n",
    "    required=False,\n",
    "    default = 'facebook/esmfold_v1',\n",
    "    help = 'Huggingface model string for Protein Langauge Model'\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--batch_size',\n",
    "    type = int,\n",
    "    required=False,\n",
    "    default=16,\n",
    "    help = 'Batch size for PLM inference'\n",
    ")\n",
    "\n",
    "args_str = '--msa_file /nfshomes/vla/cmsc702-protein-lm/data/cadherin/PF00028.alignment.seed --batch_size 16'.split()\n",
    "args = parser.parse_args(args_str)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForProteinFolding were not initialized from the model checkpoint at facebook/esmfold_v1 and are newly initialized: ['esm.contact_head.regression.bias', 'esm.contact_head.regression.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Init model\n",
    "model = EsmForProteinFolding.from_pretrained(args.plm_model)\n",
    "model.to(device)\n",
    "\n",
    "# Init tokenizer\n",
    "tokenizer =  AutoTokenizer.from_pretrained(args.plm_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PLM Inference on PF00028.alignment.seed: 100%|██████████| 4/4 [00:41<00:00, 10.37s/it]\n"
     ]
    }
   ],
   "source": [
    "contact_matrices = plm_inference_per_sequence(model, \n",
    "                            tokenizer,\n",
    "                            args.msa_file,\n",
    "                            args.batch_size,\n",
    "                            device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/nfshomes/vla/cmsc702-protein-lm/results/cadherin/esmfold'\n",
    "job_name = 'cadherin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,matrix in enumerate(contact_matrices):\n",
    "    np.save(os.path.join(output_dir, f'{i}_{job_name}.npy'), matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 90, 90, 64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bins = np.append(0,np.linspace(2.3125,21.6875,63))\n",
    "# sm_contacts = softmax(out[\"distogram_logits\"].detach().cpu(),-1)\n",
    "# sm_contacts.shape\n",
    "\n",
    "# # NOTE: pretty sure this just looks for all distances <8 angstroms?  \n",
    "# sm_contacts = sm_contacts[...,bins<8].sum(-1)\n",
    "# sm_contacts.shape\n",
    "# mask = (out[\"atom37_atom_exists\"][:,:,1] == 1).detach().cpu()\n",
    "# mask.shape\n",
    "# sm_contacts[0].shape\n",
    "# mask[0].shape\n",
    "\n",
    "# sm_contacts[0][mask[0],:][:,mask[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
