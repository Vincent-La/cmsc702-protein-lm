Some weights of EsmForProteinFolding were not initialized from the model checkpoint at facebook/esmfold_v1 and are newly initialized: ['esm.contact_head.regression.bias', 'esm.contact_head.regression.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   0%|          | 0/625 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   0%|          | 1/625 [00:12<2:06:52, 12.20s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   0%|          | 2/625 [00:24<2:09:41, 12.49s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   0%|          | 3/625 [00:36<2:07:10, 12.27s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   1%|          | 4/625 [00:47<2:00:58, 11.69s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   1%|          | 5/625 [01:00<2:06:43, 12.26s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   1%|          | 6/625 [01:11<2:00:28, 11.68s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   1%|          | 7/625 [01:22<1:59:36, 11.61s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   1%|▏         | 8/625 [01:34<2:00:27, 11.71s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   1%|▏         | 9/625 [01:45<1:57:33, 11.45s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   2%|▏         | 10/625 [01:59<2:04:08, 12.11s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   2%|▏         | 11/625 [02:11<2:02:46, 12.00s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   2%|▏         | 12/625 [02:22<1:59:08, 11.66s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   2%|▏         | 13/625 [02:34<2:02:39, 12.03s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   2%|▏         | 14/625 [02:46<2:00:14, 11.81s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   2%|▏         | 15/625 [02:59<2:03:22, 12.14s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   3%|▎         | 16/625 [03:10<2:01:13, 11.94s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   3%|▎         | 17/625 [03:22<2:01:03, 11.95s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   3%|▎         | 18/625 [03:33<1:58:17, 11.69s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   3%|▎         | 19/625 [03:45<2:00:07, 11.89s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   3%|▎         | 20/625 [03:58<2:00:43, 11.97s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   3%|▎         | 21/625 [04:09<1:57:16, 11.65s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   4%|▎         | 22/625 [04:27<2:17:23, 13.67s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   4%|▎         | 23/625 [04:39<2:12:03, 13.16s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   4%|▍         | 24/625 [04:50<2:06:15, 12.61s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   4%|▍         | 25/625 [05:02<2:02:46, 12.28s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   4%|▍         | 26/625 [05:17<2:10:24, 13.06s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   4%|▍         | 27/625 [05:28<2:04:19, 12.47s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   4%|▍         | 28/625 [05:38<1:57:58, 11.86s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   5%|▍         | 29/625 [05:49<1:55:32, 11.63s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   5%|▍         | 30/625 [06:03<2:03:08, 12.42s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   5%|▍         | 31/625 [06:14<1:58:24, 11.96s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   5%|▌         | 32/625 [06:26<1:57:36, 11.90s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   5%|▌         | 33/625 [06:38<1:56:59, 11.86s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   5%|▌         | 34/625 [06:49<1:54:02, 11.58s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   6%|▌         | 35/625 [07:01<1:55:34, 11.75s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   6%|▌         | 36/625 [07:12<1:53:25, 11.55s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   6%|▌         | 37/625 [07:23<1:52:28, 11.48s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   6%|▌         | 38/625 [07:37<1:59:51, 12.25s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   6%|▌         | 39/625 [07:50<1:59:22, 12.22s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   6%|▋         | 40/625 [08:02<1:59:34, 12.26s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   7%|▋         | 41/625 [08:14<1:57:53, 12.11s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   7%|▋         | 42/625 [08:26<1:58:27, 12.19s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   7%|▋         | 43/625 [08:38<1:57:02, 12.07s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   7%|▋         | 44/625 [08:49<1:54:40, 11.84s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   7%|▋         | 45/625 [09:03<2:00:12, 12.44s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   7%|▋         | 46/625 [09:15<1:57:18, 12.16s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   8%|▊         | 47/625 [09:25<1:53:25, 11.77s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   8%|▊         | 48/625 [09:36<1:50:41, 11.51s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   8%|▊         | 49/625 [09:48<1:52:25, 11.71s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   8%|▊         | 50/625 [10:02<1:58:17, 12.34s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   8%|▊         | 51/625 [10:15<2:00:19, 12.58s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   8%|▊         | 52/625 [10:28<1:58:53, 12.45s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   8%|▊         | 53/625 [10:38<1:53:20, 11.89s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   9%|▊         | 54/625 [10:49<1:50:16, 11.59s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   9%|▉         | 55/625 [11:00<1:48:06, 11.38s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   9%|▉         | 56/625 [11:12<1:50:45, 11.68s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   9%|▉         | 57/625 [11:23<1:48:21, 11.45s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   9%|▉         | 58/625 [11:36<1:52:53, 11.95s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:   9%|▉         | 59/625 [11:49<1:53:55, 12.08s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  10%|▉         | 60/625 [12:07<2:11:27, 13.96s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  10%|▉         | 61/625 [12:18<2:02:32, 13.04s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  10%|▉         | 62/625 [12:29<1:57:25, 12.52s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  10%|█         | 63/625 [12:41<1:54:20, 12.21s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  10%|█         | 64/625 [12:51<1:49:31, 11.71s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  10%|█         | 65/625 [13:03<1:49:23, 11.72s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  11%|█         | 66/625 [13:15<1:50:59, 11.91s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  11%|█         | 67/625 [13:27<1:49:40, 11.79s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  11%|█         | 68/625 [13:39<1:50:30, 11.90s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  11%|█         | 69/625 [13:50<1:46:38, 11.51s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  11%|█         | 70/625 [14:00<1:43:53, 11.23s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  11%|█▏        | 71/625 [14:13<1:48:16, 11.73s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  12%|█▏        | 72/625 [14:25<1:48:42, 11.80s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  12%|█▏        | 73/625 [14:36<1:45:08, 11.43s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  12%|█▏        | 74/625 [14:47<1:43:28, 11.27s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  12%|█▏        | 75/625 [14:58<1:43:57, 11.34s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  12%|█▏        | 76/625 [15:10<1:44:53, 11.46s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  12%|█▏        | 77/625 [15:21<1:44:16, 11.42s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  12%|█▏        | 78/625 [15:32<1:42:38, 11.26s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  13%|█▎        | 79/625 [15:43<1:42:32, 11.27s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  13%|█▎        | 80/625 [15:55<1:44:48, 11.54s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  13%|█▎        | 81/625 [16:08<1:46:51, 11.79s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  13%|█▎        | 82/625 [16:20<1:47:10, 11.84s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  13%|█▎        | 83/625 [16:32<1:47:51, 11.94s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  13%|█▎        | 84/625 [16:43<1:44:50, 11.63s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  14%|█▎        | 85/625 [16:53<1:41:48, 11.31s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  14%|█▍        | 86/625 [17:04<1:39:39, 11.09s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  14%|█▍        | 87/625 [17:16<1:42:54, 11.48s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  14%|█▍        | 88/625 [17:28<1:42:16, 11.43s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  14%|█▍        | 89/625 [17:38<1:39:49, 11.18s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  14%|█▍        | 90/625 [17:51<1:42:51, 11.54s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  15%|█▍        | 91/625 [18:03<1:44:18, 11.72s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  15%|█▍        | 92/625 [18:15<1:44:42, 11.79s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  15%|█▍        | 93/625 [18:27<1:45:30, 11.90s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  15%|█▌        | 94/625 [18:40<1:47:23, 12.13s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  15%|█▌        | 95/625 [18:50<1:43:07, 11.67s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  15%|█▌        | 96/625 [19:03<1:45:32, 11.97s/it]ESM-FOLD Inference on PF00028_1000SHAPE mismatch: 84 != 83
SHAPE mismatch: 83 != 82
SHAPE mismatch: 88 != 87
SHAPE mismatch: 87 != 84
SHAPE mismatch: 79 != 78
SHAPE mismatch: 94 != 93
SHAPE mismatch: 79 != 78
SHAPE mismatch: 77 != 76
SHAPE mismatch: 89 != 88
SHAPE mismatch: 85 != 83
0_msa_trimmed.faa:  16%|█▌        | 97/625 [19:14<1:42:58, 11.70s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  16%|█▌        | 98/625 [19:25<1:42:13, 11.64s/it]ESM-FOLD Inference on PF00028_10000_msa_trimmed.faa:  16%|█▌        | 98/625 [19:25<1:44:29, 11.90s/it]
Traceback (most recent call last):
  File "/fs/nexus-scratch/vla/micromamba/envs/protein-lm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 776, in convert_to_tensors
    tensor = as_tensor(value)
  File "/fs/nexus-scratch/vla/micromamba/envs/protein-lm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 738, in as_tensor
    return torch.tensor(value)
RuntimeError: Could not infer dtype of NoneType

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/nfshomes/vla/cmsc702-protein-lm/scripts/esm_inference.py", line 200, in <module>
    contact_matrices = esmfold_inference_per_sequence(model, 
  File "/nfshomes/vla/cmsc702-protein-lm/scripts/esm_inference.py", line 99, in esmfold_inference_per_sequence
    inputs = tokenizer(batch, 
  File "/fs/nexus-scratch/vla/micromamba/envs/protein-lm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3021, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/fs/nexus-scratch/vla/micromamba/envs/protein-lm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3109, in _call_one
    return self.batch_encode_plus(
  File "/fs/nexus-scratch/vla/micromamba/envs/protein-lm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3311, in batch_encode_plus
    return self._batch_encode_plus(
  File "/fs/nexus-scratch/vla/micromamba/envs/protein-lm/lib/python3.10/site-packages/transformers/tokenization_utils.py", line 892, in _batch_encode_plus
    batch_outputs = self._batch_prepare_for_model(
  File "/fs/nexus-scratch/vla/micromamba/envs/protein-lm/lib/python3.10/site-packages/transformers/tokenization_utils.py", line 979, in _batch_prepare_for_model
    batch_outputs = BatchEncoding(batch_outputs, tensor_type=return_tensors)
  File "/fs/nexus-scratch/vla/micromamba/envs/protein-lm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 240, in __init__
    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)
  File "/fs/nexus-scratch/vla/micromamba/envs/protein-lm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 792, in convert_to_tensors
    raise ValueError(
ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
srun: error: vulcan33: task 0: Exited with exit code 1
